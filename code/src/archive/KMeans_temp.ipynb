{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py Loading\n",
      "Reading HDI dataset\n",
      "co2prod: min=0.0, max=33.3863\n",
      "hdi: min=0.257, max=0.967\n",
      "le: min=37.105, max=85.473\n",
      "gdi: min=0.383, max=1.041\n",
      "eys: min=3.5751, max=23.2477\n",
      "mys: min=1.4606, max=14.2559\n",
      "\n",
      "Data scaled\n",
      "co2prod: min=0.0, max=1.0\n",
      "hdi: min=0.0, max=1.0\n",
      "le: min=0.0, max=1.0\n",
      "gdi: min=0.0, max=1.0\n",
      "eys: min=0.0, max=1.0\n",
      "mys: min=0.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from utils.utils import *\n",
    "import utils.promethee_functions as pf\n",
    "import utils.clustering_functions as cf\n",
    "\n",
    "data = read_data()\n",
    "\n",
    "group0 = [\"PAK\", \"SDN\", \"BDI\", \"HTI\"]\n",
    "group1 = [\"EST\", \"CZE\", \"MLT\", \"SGP\", \"IRL\"]\n",
    "group2 = [\"CHE\", \"ISL\", \"NZL\", \"SWE\"]\n",
    "\n",
    "all_groups = group0 + group1 + group2\n",
    "\n",
    "data = data.loc[all_groups]\n",
    "\n",
    "data = scale_data(data)\n",
    "\n",
    "print(\"\\nData scaled\")\n",
    "get_min_max_criteria(data, False)\n",
    "\n",
    "L = data.iloc[0][\"co2prod\"].shape[0] # Length of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = data.shape[0]\n",
    "n_features = data.shape[1]\n",
    "\n",
    "formatted_data = np.stack([np.stack(data.iloc[:, i].values) for i in range(n_features)], axis=-1)\n",
    "\n",
    "formatted_data.shape\n",
    "\n",
    "names = data.index\n",
    "names_formatted = [name for name in names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['MLT', 'SGP', 'IRL', 'CHE', 'ISL', 'NZL', 'SWE']\n",
      "Cluster 1: ['PAK', 'SDN', 'BDI', 'HTI']\n",
      "Cluster 2: ['EST', 'CZE']\n"
     ]
    }
   ],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "n_clusters = 3\n",
    "\n",
    "km = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"euclidean\", max_iter=5).fit(formatted_data)\n",
    "km.labels_\n",
    "\n",
    "clusters = [[] for _ in range(n_clusters)]\n",
    "for i in range(n_samples):\n",
    "    clusters[km.labels_[i]].append(names_formatted[i])\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(f\"Cluster {i}: {clusters[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Dunn index: 0.4080525707838137\n"
     ]
    }
   ],
   "source": [
    "def dunn_index_multivariate(clusters, data):\n",
    "    \"\"\" \n",
    "        Compute the Dunn index for a clustering of multivariate time series data\n",
    "        - clusters: list of lists of indexes of the time series in each cluster\n",
    "        - data: the data set (dataframe with index as id of the time series), each cell is a np.array (time series)\n",
    "    \"\"\"\n",
    "    def dunn_index_univariate(clusters, data):\n",
    "        \"\"\"\n",
    "            Compute the Dunn index for a clustering of univariate time series data\n",
    "            - clusters: list of lists of indexes of the time series in each cluster\n",
    "            - data: the data set (dataframe with index as id of the time series), only one column where each cell is a np.array (time series)\n",
    "        \"\"\"\n",
    "        centroids = []\n",
    "\n",
    "        # Define the centroids of the clusters\n",
    "        for cluster in clusters:\n",
    "            centroid = np.zeros_like(data.iloc[0])\n",
    "            for country in cluster:\n",
    "                centroid += data.loc[country]\n",
    "            centroid /= len(cluster)\n",
    "            centroids.append(centroid)\n",
    "\n",
    "        centroids = np.array(centroids)\n",
    "        \n",
    "        # Compute the distances between clusters\n",
    "        inter_cluster_distances = []\n",
    "        for i in range(len(clusters)):\n",
    "            for j in range(i+1, len(clusters)):\n",
    "                inter_cluster_distances.append(np.linalg.norm(centroids[i] - centroids[j]))\n",
    "\n",
    "        # Compute the diameter of each cluster\n",
    "        # Diameter = max distance between two countries in the cluster\n",
    "        cluster_diameters = []\n",
    "        for cluster in clusters:\n",
    "            diameter = 0\n",
    "            for i in range(len(cluster)):\n",
    "                for j in range(i+1, len(cluster)):\n",
    "                    diameter = max(diameter, np.linalg.norm(data.loc[cluster[i]] - data.loc[cluster[j]]))\n",
    "            cluster_diameters.append(diameter)\n",
    "\n",
    "        dunn_index = min(inter_cluster_distances) / max(cluster_diameters)\n",
    "        return dunn_index\n",
    "\n",
    "    indexes = {}\n",
    "    criterias = data.columns\n",
    "\n",
    "    for criteria in criterias:\n",
    "        indexes[criteria] = dunn_index_univariate(clusters, data[criteria])\n",
    "    \n",
    "    return indexes\n",
    "\n",
    "dunn_indexes = dunn_index_multivariate(clusters, data)\n",
    "\n",
    "# Get the mean of the Dunn index for each criteria\n",
    "mean_dunn_index = np.mean(list(dunn_indexes.values()))\n",
    "print(f\"Mean Dunn index: {mean_dunn_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
